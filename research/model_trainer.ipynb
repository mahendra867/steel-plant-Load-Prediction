{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\datascience End to End Projects\\\\steel-plant-Load-Prediction'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd  # this tell us which path we are currently working , so based on the below output path we are working under the research file\n",
    "os.chdir(\"c:\\\\datascience End to End Projects\\\\steel-plant-Load-Prediction\")  #  but i would like to work with main ProjectML_with_MLFlow file , so for getting i step back in path inorder to enter the main project file i used this command os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now iam creating the entity class which consist of config.yaml folder model trainer code part variables, along with that iam adding some more varaibles like alpha,l1_ratio,target_column inside my entity class\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    target_column: str  # this target column is present inside the Schema.yaml file which it tells us the quality of the Wine based on the value it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PROJECTML.constants import *\n",
    "from PROJECTML.utils.common import read_yaml, create_directories\n",
    "from PROJECTML import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this template we use for every stage like data_ingestion,data_validation,data_transformation, model trainer .. etc\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "# this is part of code for the Model trainerConfig which helps us to return the configuration\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer   # here iam reading the schema, params \n",
    "        #params = self.params.ElasticNet\n",
    "        schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "            #alpha = params.alpha,    # here from params iam taking the alpha l1_ratio\n",
    "            #l1_ratio = params.l1_ratio, \n",
    "            target_column = schema.name # here from schema iam taking the name which i will return through target_column\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config # here iam returning all variables from the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evidently\n",
    "import pandas as pd\n",
    "import os\n",
    "from PROJECTML import logger\n",
    "import joblib # here iam saving the model because i want to save the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.PROJECTML.config.configuration import ConfigurationManager\n",
    "from src.PROJECTML.components.data_transformation import DataTransformation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "#from PROJECTML.entity.config_entity import ModelTrainerConfig\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import  ExtraTreesClassifier\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported necessary libraries for model monitoring with evidently ai\n",
    "from datetime import datetime, time\n",
    "from sklearn import datasets, ensemble\n",
    "\n",
    "#!pip install evidently\n",
    "import evidently\n",
    "evidently.__version__\n",
    "from evidently.test_suite import TestSuite\n",
    "#from evidently.mertics import *\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report  # here we are importing reports which helps us to give reports of model performance w.r.t time  and data\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset, ClassificationPreset # evidently has 3 evaluation metric_preset which are DataDriftPreset, TargetDriftPreset, ClassificationPreset  so each thing has its importance , this DataDriftPreset parameter helps us to make specifically understand the DataDriftPreset, TargetDriftPreset this helps us to understand whether the target variable has changed or not,ClassificationPreset is for we are dealing with classification problem statement what performance meterics we have to monitor our model so we have precision recall f1 score confusion metrics accuracy so by importing this ClassificationPreset we can montior our model  based on whetehr regarding different performance metircs are changing w.r.t time and data \n",
    "#from evidently.ui.dashboard import Dashboard\n",
    "#from evidently.tabs import ClassificationPerformanceTab\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "from evidently.metric_preset import DataQualityPreset\n",
    "from evidently.metric_preset import TargetDriftPreset\n",
    "from evidently.metric_preset import ClassificationPreset\n",
    "data_drift_report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "])\n",
    "\n",
    "classification_performance_report = Report(metrics=[\n",
    "    ClassificationPreset(),\n",
    "])\n",
    "\n",
    "\n",
    "multiclass_cat_target_drift_report = Report(metrics=[\n",
    "    TargetDriftPreset(),\n",
    "])\n",
    "\n",
    "data_quality_report = Report(metrics=[\n",
    "    DataQualityPreset(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_data = pd.read_csv(self.config.train_data_path)\n",
    "        self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "        self.x_train = self.train_data.drop(columns=['Load_Type'])\n",
    "        self.logger.info(f'Training data loaded: Columns - {self.x_train.columns}')\n",
    "        self.y_train = self.train_data['Load_Type']\n",
    "        self.x_test = self.test_data.drop(columns=['Load_Type'])\n",
    "        self.y_test = self.test_data['Load_Type']\n",
    "\n",
    "        return self.train_data\n",
    "\n",
    "    def model_monitering(self):  # Added current as an argument\n",
    "        self.logger.info('Starting model monitoring...')\n",
    "        self.target = 'Load_Type'  # target variable\n",
    "        self.prediction = 'prediction'  # Recording the prediction values so for that i have create a  column name prediction\n",
    "        # Reference data means historical data we train the model ,Current data means upcoming or new data that we are going to train the model\n",
    "        # All features are numerical (assuming this is correct)\n",
    "        self.numerical_features = ['WeekStatus_Weekday', 'WeekStatus_Weekend', 'Usage_kWh',\n",
    "                                  'Lagging_Reactive_Power_kVarh', 'Leading_Reactive_Power_kVarh',\n",
    "                                  'CO2', 'Lagging_Power_Factor', 'Leading_Power_Factor', 'NSM', 'hour']\n",
    "        #self.categorical_features = []  # Empty list since no categorical features\n",
    "\n",
    "        self.logger.info('Training the model...')\n",
    "        # Train the ExtraTreesClassifier model\n",
    "        model = ExtraTreesClassifier()\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "\n",
    "        self.logger.info('Making predictions on test data...')\n",
    "        # Make predictions on current data\n",
    "        self.train_data['prediction'] = model.predict(self.x_train)\n",
    "        self.test_data['prediction'] = model.predict(self.x_test)\n",
    "        #print(self.test_data)\n",
    "\n",
    "        self.logger.info('Evaluating model performance...')\n",
    "        # Model performance evaluation\n",
    "        column_mapping = ColumnMapping()\n",
    "        column_mapping.target = self.target\n",
    "        column_mapping.prediction = self.prediction\n",
    "        column_mapping.numerical_features = self.numerical_features\n",
    "        classification_performance_report.run(reference_data=self.train_data, current_data=self.test_data, column_mapping=column_mapping)  # i passed reference data as trained data with i made prediction and store them in this trained traine_data and i have consider the test_data as upcoming data or current data which i made prediction so i passed these train and test data as historical data and upcoming data then we can observe the data drift and model drift that how our model and data is getting flutuate by we monitor that thing by this evidently ai tool by report\n",
    "        data_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        data_quality_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        multiclass_cat_target_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        \n",
    "        data_drift_report.save_html(\"data_drift_file.html\")\n",
    "        classification_performance_report.save_html('Classification report.html')\n",
    "        data_quality_report.save_html(\"Data_quality_report.html\")\n",
    "        multiclass_cat_target_drift_report.save_html(\"target_drift_report.html\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        self.train_data = pd.read_csv(self.config.train_data_path)\n",
    "        self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        self.x_train = self.train_data.drop(columns=['Load_Type'])\n",
    "        print(f'this is the self.x_train dataset {self.x_train.columns}')\n",
    "        self.y_train = self.train_data['Load_Type']\n",
    "        self.x_test = self.test_data.drop(columns=['Load_Type'])\n",
    "        self.y_test = self.test_data['Load_Type']\n",
    "\n",
    "    def model_monitering(self):  # Added current as an argument\n",
    "        \n",
    "        self.target = 'Load_Type'  \n",
    "        self.prediction = 'prediction'  \n",
    "        self.numerical_features = ['WeekStatus_Weekday', 'WeekStatus_Weekend', 'Usage_kWh',\n",
    "                                  'Lagging_Reactive_Power_kVarh', 'Leading_Reactive_Power_kVarh',\n",
    "                                  'CO2', 'Lagging_Power_Factor', 'Leading_Power_Factor', 'NSM', 'hour']\n",
    "        #self.categorical_features = []  # Empty list since no categorical features\n",
    "\n",
    "        \n",
    "        # Train the ExtraTreesClassifier model\n",
    "        self.model = ExtraTreesClassifier()\n",
    "        self.model.fit(self.x_train, self.y_train)\n",
    "\n",
    "        \n",
    "        # Make predictions on current data\n",
    "        self.train_data['prediction'] = self.model.predict(self.x_train) \n",
    "        self.test_data['prediction'] = self.model.predict(self.x_test)  \n",
    "\n",
    "        self.train_pred=self.train_data['prediction']\n",
    "        self.test_pred=self.test_data['prediction']\n",
    "        #print(self.test_data)\n",
    "\n",
    "        # Model performance evaluation\n",
    "        column_mapping = ColumnMapping()\n",
    "        column_mapping.target = self.target\n",
    "        column_mapping.prediction = self.prediction\n",
    "        column_mapping.numerical_features = self.numerical_features\n",
    "        classification_performance_report.run(reference_data=self.train_data, current_data=self.test_data, column_mapping=column_mapping)  # i passed reference data as trained data with i made prediction and store them in this trained traine_data and i have consider the test_data as upcoming data or current data which i made prediction so i passed these train and test data as historical data and upcoming data then we can observe the data drift and model drift that how our model and data is getting flutuate by we monitor that thing by this evidently ai tool by report\n",
    "        data_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        data_quality_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        multiclass_cat_target_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        data_drift_report.save_html(\"data_drift_file.html\")\n",
    "        classification_performance_report.save_html('Classification report.html')\n",
    "        data_quality_report.save_html(\"Data_quality_report.html\")\n",
    "        multiclass_cat_target_drift_report.save_html(\"target_drift_report.html\")\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_model()(self):\n",
    "        model=self.model\n",
    "        train_accuracy = accuracy_score(self.y_train, self.train_pred)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.test_pred)\n",
    "\n",
    "        train_cm = confusion_matrix(self.y_train, self.train_pred)\n",
    "        test_cm = confusion_matrix(self.y_test, self.test_pred)\n",
    "\n",
    "        train_precision = precision_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_precision = precision_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        train_recall = recall_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_recall = recall_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        train_f1 = f1_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_f1 = f1_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        scores={\n",
    "                'Training Accuracy': train_accuracy,\n",
    "                'Testing Accuracy': test_accuracy,\n",
    "                'Training Precision': train_precision,\n",
    "                'Testing Precision': test_precision,\n",
    "                'Training Recall': train_recall,\n",
    "                'Testing Recall': test_recall,\n",
    "                'Training F1-score': train_f1,\n",
    "                'Testing F1-score': test_f1\n",
    "                }\n",
    "\n",
    "        for metric, value in scores.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "        joblib.dump(model, os.path.join(self.config.root_dir, f\"{type(model).__name__}_model.joblib\"))\n",
    "\n",
    "\n",
    "                # Load the trained model  and test model \n",
    "        model = joblib.load(\"artifacts\\model_trainer\\ExtraTreesClassifier_model.joblib\")  # Replace \"path_to_saved_model.pkl\" with the actual path\n",
    "\n",
    "            #self.preprocessor = joblib.load('artifacts\\data_transformation\\categorical_preprocessor_obj.joblib')\n",
    "            # Prepare input data for prediction (a single sample row)\n",
    "            # Replace the feature values with the values of your unseen test data\n",
    "        single_sample = {\n",
    "            'Usage_kWh': 8.46,\n",
    "            'Lagging_Reactive_Power_kVarh': 0,\n",
    "            'Leading_Reactive_Power_kVarh': 25.92,\n",
    "            'CO2': 0,\n",
    "            'Lagging_Power_Factor': 100,\n",
    "            'Leading_Power_Factor': 31.03,\n",
    "            'NSM': 45000,\n",
    "            'WeekStatus_Weekday': 1,\n",
    "            'WeekStatus_Weekend': 0,\n",
    "            'hour': 20\n",
    "            \n",
    "        }\n",
    "                    \n",
    "        #8.46,0,25.92,0,100,31.03,45000,Weekday,Tuesday,Medium_Load\n",
    "\n",
    "        #40.25,8.82,0.5,0,97.68,99.99,67500,Weekday,Tuesday,Maximum_Load\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "        input_data = pd.DataFrame([single_sample])\n",
    "        #preprocessed_input_data = self.preprocessor.transform(input_data)\n",
    "\n",
    "        # Ensure that the columns of input_data match the order of features used during training\n",
    "        # You might need to rearrange the columns or add missing columns\n",
    "        input_data = input_data[self.x_train.columns]\n",
    "\n",
    "        # Perform prediction\n",
    "        prediction = model.predict(input_data)\n",
    "\n",
    "        print(\"Predicted class label:\", prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        self.train_data = pd.read_csv(self.config.train_data_path)\n",
    "        self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        self.x_train = self.train_data.drop(columns=['Load_Type'])\n",
    "        print(f'this is the self.x_train dataset {self.x_train.columns}')\n",
    "        self.y_train = self.train_data['Load_Type']\n",
    "        self.x_test = self.test_data.drop(columns=['Load_Type'])\n",
    "        self.y_test = self.test_data['Load_Type']\n",
    "\n",
    "    # model monitering by evidently ai open source tool\n",
    "    def model_monitering(self):  \n",
    "        \n",
    "        self.target = 'Load_Type'   # target variable # Recording the prediction values so for that i have create a  column name prediction\n",
    "        # Reference data means historical data we train the model ,Current data means upcoming or new data that we are going to train the model\n",
    "        # All features are numerical (assuming this is correct)\n",
    "        self.prediction = 'prediction'  \n",
    "        \n",
    "        self.numerical_features = ['WeekStatus_Weekday', 'WeekStatus_Weekend', 'Usage_kWh',\n",
    "                                  'Lagging_Reactive_Power_kVarh', 'Leading_Reactive_Power_kVarh',\n",
    "                                  'CO2', 'Lagging_Power_Factor', 'Leading_Power_Factor', 'NSM', 'hour']\n",
    "\n",
    "        self.model = ExtraTreesClassifier()\n",
    "        self.model.fit(self.x_train, self.y_train)\n",
    "\n",
    "        self.train_data['prediction'] = self.model.predict(self.x_train) # iam considering this self.train_data as historical data or reference data \n",
    "        self.test_data['prediction'] = self.model.predict(self.x_test)   # iam considering this self.test_data as upcoming data or current new data to compare performance with reference \n",
    "\n",
    "        self.train_pred=self.train_data['prediction']\n",
    "        self.test_pred=self.test_data['prediction']\n",
    "\n",
    "        column_mapping = ColumnMapping()\n",
    "        column_mapping.target = self.target\n",
    "        column_mapping.prediction = self.prediction\n",
    "        column_mapping.numerical_features = self.numerical_features\n",
    "\n",
    "        classification_performance_report.run(reference_data=self.train_data, current_data=self.test_data, column_mapping=column_mapping)  \n",
    "        data_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        data_quality_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "        multiclass_cat_target_drift_report.run(current_data=self.test_data, reference_data=self.train_data, column_mapping=None)\n",
    "\n",
    "        data_drift_report.save_html(os.path.join(self.config.root_dir, \"data_drift_file.html\"))\n",
    "        classification_performance_report.save_html(os.path.join(self.config.root_dir, \"Classification_report.html\"))\n",
    "        data_quality_report.save_html(os.path.join(self.config.root_dir, \"Data_quality_report.html\"))\n",
    "        multiclass_cat_target_drift_report.save_html(os.path.join(self.config.root_dir, \"target_drift_report.html\"))\n",
    "        \n",
    "        #data_drift_report.save_html(\"data_drift_file.html\")\n",
    "        #classification_performance_report.save_html('Classification report.html')\n",
    "        #data_quality_report.save_html(\"Data_quality_report.html\")\n",
    "        #multiclass_cat_target_drift_report.save_html(\"target_drift_report.html\")\n",
    "\n",
    "    def evaluate_model(self):  # Renamed method to evaluate_model\n",
    "        model=self.model\n",
    "        train_accuracy = accuracy_score(self.y_train, self.train_pred)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.test_pred)\n",
    "\n",
    "        train_cm = confusion_matrix(self.y_train, self.train_pred)\n",
    "        test_cm = confusion_matrix(self.y_test, self.test_pred)\n",
    "\n",
    "        train_precision = precision_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_precision = precision_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        train_recall = recall_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_recall = recall_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        train_f1 = f1_score(self.y_train, self.train_pred, average='weighted')\n",
    "        test_f1 = f1_score(self.y_test, self.test_pred, average='weighted')\n",
    "\n",
    "        scores={\n",
    "                'Model': type(model).__name__,\n",
    "                'Training Accuracy': train_accuracy,\n",
    "                'Testing Accuracy': test_accuracy,\n",
    "                'Training Precision': train_precision,\n",
    "                'Testing Precision': test_precision,\n",
    "                'Training Recall': train_recall,\n",
    "                'Testing Recall': test_recall,\n",
    "                'Training F1-score': train_f1,\n",
    "                'Testing F1-score': test_f1\n",
    "                }\n",
    "\n",
    "        for metric, value in scores.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "        joblib.dump(model, os.path.join(self.config.root_dir, f\"{type(model).__name__}_model.joblib\"))\n",
    "\n",
    "        model = joblib.load(\"artifacts\\model_trainer\\ExtraTreesClassifier_model.joblib\")  \n",
    "\n",
    "        single_sample = {\n",
    "            'Usage_kWh': 8.46,\n",
    "            'Lagging_Reactive_Power_kVarh': 0,\n",
    "            'Leading_Reactive_Power_kVarh': 25.92,\n",
    "            'CO2': 0,\n",
    "            'Lagging_Power_Factor': 100,\n",
    "            'Leading_Power_Factor': 31.03,\n",
    "            'NSM': 45000,\n",
    "            'WeekStatus_Weekday': 1,\n",
    "            'WeekStatus_Weekend': 0,\n",
    "            'hour': 20\n",
    "        }\n",
    "                    \n",
    "        input_data = pd.DataFrame([single_sample])\n",
    "        input_data = input_data[self.x_train.columns]\n",
    "\n",
    "        prediction = model.predict(input_data)\n",
    "\n",
    "        print(\"Predicted class label:\", prediction)\n",
    "\n",
    "        return pd.DataFrame(scores, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 00:08:57,424: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-04-20 00:08:57,440: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-04-20 00:08:57,446: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-04-20 00:08:57,451: INFO: common: created directory at: artifacts]\n",
      "[2024-04-20 00:08:57,455: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the self.x_train dataset Index(['WeekStatus_Weekday', 'WeekStatus_Weekend', 'Usage_kWh',\n",
      "       'Lagging_Reactive_Power_kVarh', 'Leading_Reactive_Power_kVarh', 'CO2',\n",
      "       'Lagging_Power_Factor', 'Leading_Power_Factor', 'NSM', 'hour'],\n",
      "      dtype='object')\n",
      "Model: ExtraTreesClassifier\n",
      "Training Accuracy: 0.9996802911809551\n",
      "Testing Accuracy: 0.9550686144311642\n",
      "Training Precision: 0.9996802803009861\n",
      "Testing Precision: 0.9552925558070994\n",
      "Training Recall: 0.9996802911809551\n",
      "Testing Recall: 0.9550686144311642\n",
      "Training F1-score: 0.9996802821192954\n",
      "Testing F1-score: 0.9550417079897668\n",
      "Predicted class label: [2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # here iam initializing my ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config() # here iam getting my get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config) # here iam passing my model_trainer_config to the ModelTrainer function\n",
    "    model_trainer.train() # here iam training the model\n",
    "    model_trainer.model_monitering()\n",
    "    model_trainer.evaluate_model()\n",
    "except Exception as e:\n",
    "    logger.exception(\"Exception occurred\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steel_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
