{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\datascience End to End Projects\\\\steel-plant-Load-Prediction-'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd  # this tell us which path we are currently working , so based on the below output path we are working under the research file\n",
    "os.chdir(\"c:\\\\datascience End to End Projects\\\\steel-plant-Load-Prediction-\")  #  but i would like to work with main ProjectML_with_MLFlow file , so for getting i step back in path inorder to enter the main project file i used this command os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called the entity \n",
    "from dataclasses import dataclass # here i imported the dataclass from the dataclasses\n",
    "from pathlib import Path  # here i imported path from pathlib\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path  # these are variables which are present inside the config.yaml file data_transformation code part and here iam mentioning inside the entity of the class\n",
    "    data_path: Path\n",
    "    preprocessor_obj: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PROJECTML.constants import *\n",
    "from PROJECTML.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is same part of the code in every step \n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    # only this part get changes in every step, only defining the get_data_transformation_config get changes according to which step we are performing like 01_data_ingestion,02_data_validation\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,  # here iam returning these 2 varaibles by using this code \n",
    "            data_path=config.data_path,\n",
    "            preprocessor_obj=config.preprocessor_obj\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PROJECTML import logger\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.compose import make_column_transformer\n",
    "from PROJECTML.config.configuration import DataTransformationConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i defined the component of DataTransformationConfig below\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "\n",
    "    def creating_new_renamed_columns_dataset(self):\n",
    "        self.dataset=self.config.data_path\n",
    "        self.new_data=pd.read_csv(self.dataset)\n",
    "        logger.info(\"loaded the dataset successfully\")\n",
    "        #Rename some columns\n",
    "        self.new_data= self.new_data.rename(columns={'Lagging_Current_Reactive.Power_kVarh': 'Lagging_Reactive_Power_kVarh',\n",
    "                                'Leading_Current_Reactive_Power_kVarh': 'Leading_Reactive_Power_kVarh',\n",
    "                                'Lagging_Current_Power_Factor': 'Lagging_Power_Factor',\n",
    "                                'Leading_Current_Power_Factor': 'Leading_Power_Factor',\n",
    "                                'CO2(tCO2)':'CO2'})\n",
    "        logger.info(\"renamed the dataset columns successfully\")\n",
    "\n",
    "\n",
    "        # Assuming self.new_data is your DataFrame containing the target feature column 'Load_Type'\n",
    "\n",
    "        # Define the oversampler\n",
    "        oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = self.new_data.drop(columns=['Load_Type'])  # Features\n",
    "        y = self.new_data['Load_Type']  # Target\n",
    "\n",
    "        # Perform random oversampling\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "        # Convert back to DataFrame if needed\n",
    "        self.new_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        self.new_data['Load_Type'] = y_resampled\n",
    "\n",
    "        # Check the distribution after oversampling\n",
    "        print(self.new_data['Load_Type'].value_counts())\n",
    "        print(self.new_data.head())\n",
    "\n",
    "\n",
    "        #self.new_data.to_csv(os.path.join(self.config.root_dir, \"renamed_columns_dataset.csv\"),index = False)\n",
    "        \n",
    "        self.new_data.head()\n",
    "        self.new_data['date'] = pd.to_datetime(self.new_data['date'], format='%d/%m/%Y %H:%M')\n",
    "        #self.new_data['date_year'] = self.new_data['date'].dt.year  # iam dropping it because it is a constant feature \n",
    "        #self.new_data['date_month_no'] = self.new_data['date'].dt.month # iam dropping this feature because annova test suggested me to give the least for this feature because it got very less value of annova test\n",
    "        #self.new_data['date_day'] = self.new_data['date'].dt.day # same applies here\n",
    "        self.new_data['hour'] = self.new_data['date'].dt.hour\n",
    "        #self.new_data['min'] = self.new_data['date'].dt.minute  # same applies here \n",
    "        self.new_data.drop(columns='date', inplace=True)# we got extract the imp features from this date so thatsy iam dropping this date column]\n",
    "        self.new_data.drop(columns='Day_of_week', inplace=True) # iam removing this i already performed the annova test there it suggest me to not select this feature based on statistical measure thatsy iam dropping this feature\n",
    "        self.new_data.info()\n",
    "        \n",
    "\n",
    "    def pipeline_creation(self):\n",
    "        self.new_data1 = self.new_data.copy()\n",
    "        self.new_data1.drop(columns='Load_Type', inplace=True)\n",
    "\n",
    "        # Numerical columns\n",
    "        numerical_columns = self.new_data1.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Categorical columns\n",
    "        categorical_columns = ['WeekStatus']\n",
    "\n",
    "        # Apply One-Hot Encoding for categorical columns\n",
    "        categorical_pipeline = make_column_transformer(\n",
    "            (OneHotEncoder(), categorical_columns),\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # Fit and transform the categorical pipeline\n",
    "        X_transformed = categorical_pipeline.fit_transform(self.new_data1)\n",
    "\n",
    "        # Save the preprocessor object for categorical features\n",
    "        joblib.dump(categorical_pipeline, os.path.join(self.config.root_dir, \"categorical_preprocessor_obj.joblib\"))\n",
    "\n",
    "        # Label encode the target variable\n",
    "        le = LabelEncoder()\n",
    "        self.new_data['Load_Type'] = le.fit_transform(self.new_data['Load_Type'])\n",
    "\n",
    "        # Print the classes that the LabelEncoder has seen\n",
    "        print(\"Labels that were encoded:\", le.classes_)\n",
    "\n",
    "        # Print the corresponding encoded values\n",
    "        print(\"Encoded values:\", np.arange(len(le.classes_)))\n",
    "\n",
    "        # Save the label encoder object for the target variable\n",
    "        joblib.dump(le, os.path.join(self.config.root_dir, \"label_encoder_obj.joblib\"))\n",
    "\n",
    "        #label_mapping = dict(zip(self.new_data['Load_Type'], self.new_data['Load_Type_encoded']))\n",
    "        #logger.info(\"Label Encoding Mapping:\")\n",
    "        # Print the classes that the LabelEncoder has seen\n",
    "        print(\"Labels that were encoded:\", le.classes_)\n",
    "\n",
    "        # Get the feature names after one-hot encoding\n",
    "        transformed_columns = categorical_pipeline.named_transformers_['onehotencoder'].get_feature_names_out(input_features=categorical_columns).tolist()\n",
    "\n",
    "        # Combine transformed features with numerical columns\n",
    "        transformed_columns += numerical_columns\n",
    "\n",
    "        # Convert X_transformed to DataFrame\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=transformed_columns)\n",
    "\n",
    "        # Combine transformed features with target variable\n",
    "        self.new_data = pd.concat([X_transformed_df, self.new_data['Load_Type']], axis=1)\n",
    "\n",
    "        # Print transformed dataset info\n",
    "        logger.info(\"Transformed Dataset Info:\")\n",
    "        logger.info(self.new_data.info())\n",
    "\n",
    "        # Print shape and head of the transformed dataset\n",
    "        logger.info(\"Shape of Transformed Dataset:\")\n",
    "        logger.info(self.new_data.shape)\n",
    "        logger.info(\"Head of Transformed Dataset:\")\n",
    "        logger.info(self.new_data.head())\n",
    "\n",
    "\n",
    "\n",
    "    def find_constant_features(self):\n",
    "        # Assuming self.new_data is your DataFrame containing all features\n",
    "\n",
    "        # Initialize VarianceThreshold with the threshold\n",
    "        vt = VarianceThreshold(threshold=0)\n",
    "\n",
    "        # Fit the VarianceThreshold to identify constant features\n",
    "        vt.fit(self.new_data)\n",
    "\n",
    "        # Get boolean mask of features that are not constant\n",
    "        mask = vt.get_support()\n",
    "\n",
    "        # Get the list of constant features\n",
    "        constant_features = self.new_data.columns[~mask].tolist()\n",
    "\n",
    "        # Print or log the constant features\n",
    "        logger.info(\"Constant Features:\")\n",
    "        logger.info(constant_features)\n",
    "\n",
    "        return constant_features\n",
    "\n",
    "    def find_quasi_constant_features(self):\n",
    "        # Assuming self.new_data is your DataFrame containing all features\n",
    "\n",
    "        # Remove the constant features before identifying quasi-constant features\n",
    "        self.new_data = self.new_data.drop(columns=self.find_constant_features())\n",
    "\n",
    "        # Initialize VarianceThreshold with the threshold\n",
    "        vt = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "        # Fit the VarianceThreshold to identify quasi-constant features\n",
    "        vt.fit(self.new_data)\n",
    "\n",
    "        # Get boolean mask of features that are not quasi-constant\n",
    "        mask = vt.get_support()\n",
    "\n",
    "        # Get the list of quasi-constant features\n",
    "        quasi_constant_features = self.new_data.columns[~mask].tolist()\n",
    "\n",
    "        # Print or log the quasi-constant features\n",
    "        logger.info(\"Quasi-Constant Features:\")\n",
    "        logger.info(quasi_constant_features)\n",
    "\n",
    "        return quasi_constant_features\n",
    "    \n",
    "    def perform_anova_test(self):\n",
    "        # Assuming self.new_data is your DataFrame containing the target feature column 'Load_Type'\n",
    "        # and independent numerical features\n",
    "\n",
    "        # Select numerical columns\n",
    "        numerical_columns = self.new_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        print(numerical_columns)\n",
    "\n",
    "        # Perform ANOVA test for each numerical feature\n",
    "        f_values, p_values = f_classif(self.new_data[numerical_columns], self.new_data['Load_Type'])\n",
    "\n",
    "        # Create a DataFrame to store results\n",
    "        anova_results = pd.DataFrame({'Feature': numerical_columns, 'F-value': f_values, 'p-value': p_values})\n",
    "\n",
    "        # Sort the results based on F-values\n",
    "        anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "        # Print or log ANOVA results\n",
    "        logger.info(\"ANOVA Test Results:\")\n",
    "        logger.info(anova_results)\n",
    "\n",
    "        return anova_results\n",
    "    \n",
    "    #def selecting_the_best_features(self):\n",
    "    #   features_to_drop = ['date_day', 'date_month_no', 'min']\n",
    "    #   self.new_data.drop(columns=features_to_drop, inplace=True)\n",
    "            \n",
    "        \n",
    "    def train_test_spliting(self):\n",
    "       \n",
    "        transformed_dataset=self.new_data\n",
    "        \n",
    "        train, test = train_test_split(transformed_dataset,test_size=0.25,random_state=42) # this line splits the data into train_test_split\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False) # here it saves the train and test data in csv format inisde the artifacts-> transformation folder\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
    "\n",
    "        logger.info(\"Splited data into training and test sets\")\n",
    "        logger.info(train.shape) # this logs the information about that how many training and testing samples i have \n",
    "        logger.info(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-16 21:15:06,168: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-04-16 21:15:06,171: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-04-16 21:15:06,176: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-04-16 21:15:06,178: INFO: common: created directory at: artifacts]\n",
      "[2024-04-16 21:15:06,181: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-04-16 21:15:06,251: INFO: 2960569452: loaded the dataset successfully]\n",
      "[2024-04-16 21:15:06,256: INFO: 2960569452: renamed the dataset columns successfully]\n",
      "Load_Type\n",
      "Light_Load      18072\n",
      "Medium_Load     18072\n",
      "Maximum_Load    18072\n",
      "Name: count, dtype: int64\n",
      "               date  Usage_kWh  Lagging_Reactive_Power_kVarh  \\\n",
      "0  01/01/2018 00:15       3.17                          2.95   \n",
      "1  01/01/2018 00:30       4.00                          4.46   \n",
      "2  01/01/2018 00:45       3.24                          3.28   \n",
      "3  01/01/2018 01:00       3.31                          3.56   \n",
      "4  01/01/2018 01:15       3.82                          4.50   \n",
      "\n",
      "   Leading_Reactive_Power_kVarh  CO2  Lagging_Power_Factor  \\\n",
      "0                           0.0  0.0                 73.21   \n",
      "1                           0.0  0.0                 66.77   \n",
      "2                           0.0  0.0                 70.28   \n",
      "3                           0.0  0.0                 68.09   \n",
      "4                           0.0  0.0                 64.72   \n",
      "\n",
      "   Leading_Power_Factor   NSM WeekStatus Day_of_week   Load_Type  \n",
      "0                 100.0   900    Weekday      Monday  Light_Load  \n",
      "1                 100.0  1800    Weekday      Monday  Light_Load  \n",
      "2                 100.0  2700    Weekday      Monday  Light_Load  \n",
      "3                 100.0  3600    Weekday      Monday  Light_Load  \n",
      "4                 100.0  4500    Weekday      Monday  Light_Load  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54216 entries, 0 to 54215\n",
      "Data columns (total 10 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Usage_kWh                     54216 non-null  float64\n",
      " 1   Lagging_Reactive_Power_kVarh  54216 non-null  float64\n",
      " 2   Leading_Reactive_Power_kVarh  54216 non-null  float64\n",
      " 3   CO2                           54216 non-null  float64\n",
      " 4   Lagging_Power_Factor          54216 non-null  float64\n",
      " 5   Leading_Power_Factor          54216 non-null  float64\n",
      " 6   NSM                           54216 non-null  int64  \n",
      " 7   WeekStatus                    54216 non-null  object \n",
      " 8   Load_Type                     54216 non-null  object \n",
      " 9   hour                          54216 non-null  int32  \n",
      "dtypes: float64(6), int32(1), int64(1), object(2)\n",
      "memory usage: 3.9+ MB\n",
      "Labels that were encoded: ['Light_Load' 'Maximum_Load' 'Medium_Load']\n",
      "Encoded values: [0 1 2]\n",
      "Labels that were encoded: ['Light_Load' 'Maximum_Load' 'Medium_Load']\n",
      "[2024-04-16 21:15:06,680: INFO: 2960569452: Transformed Dataset Info:]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54216 entries, 0 to 54215\n",
      "Data columns (total 11 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   WeekStatus_Weekday            54216 non-null  float64\n",
      " 1   WeekStatus_Weekend            54216 non-null  float64\n",
      " 2   Usage_kWh                     54216 non-null  float64\n",
      " 3   Lagging_Reactive_Power_kVarh  54216 non-null  float64\n",
      " 4   Leading_Reactive_Power_kVarh  54216 non-null  float64\n",
      " 5   CO2                           54216 non-null  float64\n",
      " 6   Lagging_Power_Factor          54216 non-null  float64\n",
      " 7   Leading_Power_Factor          54216 non-null  float64\n",
      " 8   NSM                           54216 non-null  float64\n",
      " 9   hour                          54216 non-null  float64\n",
      " 10  Load_Type                     54216 non-null  int32  \n",
      "dtypes: float64(10), int32(1)\n",
      "memory usage: 4.3 MB\n",
      "[2024-04-16 21:15:06,687: INFO: 2960569452: None]\n",
      "[2024-04-16 21:15:06,688: INFO: 2960569452: Shape of Transformed Dataset:]\n",
      "[2024-04-16 21:15:06,692: INFO: 2960569452: (54216, 11)]\n",
      "[2024-04-16 21:15:06,693: INFO: 2960569452: Head of Transformed Dataset:]\n",
      "[2024-04-16 21:15:06,694: INFO: 2960569452:    WeekStatus_Weekday  WeekStatus_Weekend  Usage_kWh  \\\n",
      "0                 1.0                 0.0       3.17   \n",
      "1                 1.0                 0.0       4.00   \n",
      "2                 1.0                 0.0       3.24   \n",
      "3                 1.0                 0.0       3.31   \n",
      "4                 1.0                 0.0       3.82   \n",
      "\n",
      "   Lagging_Reactive_Power_kVarh  Leading_Reactive_Power_kVarh  CO2  \\\n",
      "0                          2.95                           0.0  0.0   \n",
      "1                          4.46                           0.0  0.0   \n",
      "2                          3.28                           0.0  0.0   \n",
      "3                          3.56                           0.0  0.0   \n",
      "4                          4.50                           0.0  0.0   \n",
      "\n",
      "   Lagging_Power_Factor  Leading_Power_Factor     NSM  hour  Load_Type  \n",
      "0                 73.21                 100.0   900.0   0.0          0  \n",
      "1                 66.77                 100.0  1800.0   0.0          0  \n",
      "2                 70.28                 100.0  2700.0   0.0          0  \n",
      "3                 68.09                 100.0  3600.0   1.0          0  \n",
      "4                 64.72                 100.0  4500.0   1.0          0  ]\n",
      "[2024-04-16 21:15:06,725: INFO: 2960569452: Constant Features:]\n",
      "[2024-04-16 21:15:06,726: INFO: 2960569452: []]\n",
      "[2024-04-16 21:15:06,742: INFO: 2960569452: Constant Features:]\n",
      "[2024-04-16 21:15:06,744: INFO: 2960569452: []]\n",
      "[2024-04-16 21:15:06,764: INFO: 2960569452: Quasi-Constant Features:]\n",
      "[2024-04-16 21:15:06,765: INFO: 2960569452: ['CO2']]\n",
      "['WeekStatus_Weekday', 'WeekStatus_Weekend', 'Usage_kWh', 'Lagging_Reactive_Power_kVarh', 'Leading_Reactive_Power_kVarh', 'CO2', 'Lagging_Power_Factor', 'Leading_Power_Factor', 'NSM', 'hour', 'Load_Type']\n",
      "[2024-04-16 21:15:06,793: INFO: 2960569452: ANOVA Test Results:]\n",
      "[2024-04-16 21:15:06,795: INFO: 2960569452:                          Feature       F-value  p-value\n",
      "10                     Load_Type           inf      0.0\n",
      "9                           hour  1.719864e+04      0.0\n",
      "8                            NSM  1.718651e+04      0.0\n",
      "6           Lagging_Power_Factor  1.690503e+04      0.0\n",
      "2                      Usage_kWh  1.458952e+04      0.0\n",
      "5                            CO2  1.433602e+04      0.0\n",
      "3   Lagging_Reactive_Power_kVarh  7.231048e+03      0.0\n",
      "4   Leading_Reactive_Power_kVarh  3.039333e+03      0.0\n",
      "7           Leading_Power_Factor  2.579309e+03      0.0\n",
      "0             WeekStatus_Weekday  1.314685e+03      0.0\n",
      "1             WeekStatus_Weekend  1.314685e+03      0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahen\\ANACONDA\\envs\\steel_venv\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-16 21:15:07,210: INFO: 2960569452: Splited data into training and test sets]\n",
      "[2024-04-16 21:15:07,212: INFO: 2960569452: (40662, 11)]\n",
      "[2024-04-16 21:15:07,213: INFO: 2960569452: (13554, 11)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # here iam initlizing my ConfigurationManager\n",
    "    data_transformation_config = config.get_data_transformation_config() # and here iam getting my get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config) # here iam passing my data_transformation_config it means iam calling this data_transformation_config\n",
    "    data_transformation.creating_new_renamed_columns_dataset()\n",
    "    data_transformation.pipeline_creation()\n",
    "    data_transformation.find_constant_features()\n",
    "    data_transformation.find_quasi_constant_features()\n",
    "    data_transformation.perform_anova_test()\n",
    "    #data_transformation.selecting_the_best_features()\n",
    "    data_transformation.train_test_spliting() # here performing the train_test_split()\n",
    "except Exception as e: # this part of code will raise error if anything goes wrong\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steel_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
