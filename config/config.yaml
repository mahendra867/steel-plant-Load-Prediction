artifacts_root: artifacts # so basically when we are working excuting the training pipelines it creates one folder which is called artifacts which this artifacts it insides saves the data ingestion ,data validation , data tarnsformation , model trainer, model evaluation and everything   


data_ingestion: # here iam defining the data_ingestion related configuration
  root_dir: artifacts/data_ingestion  # here iam creating one data_ingestion folder inside the artifacts
  source_URL: https://github.com/mahendra867/random_datasets/raw/main/steel_plant.zip  # this is my data downloaded URL  by this if we past the URL in the google kaggle winequality dataset get downloaded
  local_data_file: artifacts/data_ingestion/data.zip # when the above data which is present in the line get downloaded it do save inside the local_data_file with name of data.zip
  unzip_dir: artifacts/data_ingestion # again iam unzipping the data file and sending it into data_ingestion

data_validation:  # here iam defining the data_ingestion related configuration
  root_dir: artifacts/data_validation  # # here iam creating one data_ingestion folder inside the artifacts
  unzip_data_dir: artifacts/data_ingestion/Steel_industry_data.csv # whichever the dataset got inside the artifacts by data_ingestion now i have given the path which consist of that csv file so our data_validation can read that dataset
  STATUS_FILE: artifacts/data_validation/status.txt # here iam keeping the file inside the data_validation , because whenever it do validation of the data if the data found as correct then our validation status of file return TRUE else or if data is not in correct format then  it will return validation_status as  False , if data is not in correct format it will not start the tarining pipeline because it need computation cost , so before training the we do validate our data which it returns the status of the dataset


# so this is my data_transformation related configuration and this will creating 1 folder inside the artifacts which is related to data_transformation
data_transformation:
  root_dir: artifacts/data_transformation
  data_path: artifacts/data_ingestion/Steel_industry_data.csv # and this is the path of the dataset , because whenever iam reading the dataset inside the data_transformation i will need this path at that time  
  preprocessor_obj: preprocessor_object_file.joblib


# here iam prepraing my model trainer configuration 
model_trainer:
  root_dir: artifacts/model_trainer  # here actually iam creating folder inside the model_trainer folder 
  train_data_path: artifacts/data_transformation/train.csv  # our model is taking the train and test path which are present inside the data_transformation
  test_data_path: artifacts/data_transformation/test.csv 
  model_name: model.joblib  # once the model got trained by the above train path our model will get saved inside the artifacts->model_trainer folder and we are saving our model in the formate of joblib rather than pickel formate because joblib is better than pickle formate